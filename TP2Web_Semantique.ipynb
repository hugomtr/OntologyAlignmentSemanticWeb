{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP2Web_Semantique.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UDydU0Btc1MX",
        "7HqJTiMm2IHF",
        "G3bS-RgCbgyz",
        "17FmZ7OGJ2mj"
      ],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "eNLLyZXlaulI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing missing packages & Loading"
      ],
      "metadata": {
        "id": "GOOCjGyMFGkG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4hU6I-YXHlP"
      },
      "outputs": [],
      "source": [
        "!pip install rdflib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install kglab\n",
        "!pip install textdistance\n",
        "!pip install textdistance[extras]\n",
        "!pip install deep_translator"
      ],
      "metadata": {
        "id": "wb3KRHAVdxT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "mudBd3woZpr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph\n",
        "from rdflib import URIRef\n",
        "from rdflib.namespace import RDF\n",
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textdistance as td\n",
        "import itertools    \n",
        "import collections"
      ],
      "metadata": {
        "id": "Xsexm5awcNsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "pGDQdlriZU4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_local_drive='/content/gdrive/My Drive/Colab Notebooks/WebSemantique'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd"
      ],
      "metadata": {
        "id": "pjKa6SpSZbwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_graph = Graph()\n",
        "target_graph.parse('target.ttl', format='ttl')\n",
        "np_arr_target = np.array(target_graph).astype(\"str\")"
      ],
      "metadata": {
        "id": "gBrGLcS1X04w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_graph = Graph()\n",
        "source_graph.parse('source.ttl', format='ttl')\n",
        "np_arr_source = np.array(source_graph).astype(\"str\")"
      ],
      "metadata": {
        "id": "DMc9R_0AlRVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_subject_source = np.unique(np_arr_source[:,0])\n",
        "# all_subject_target = np.unique(np_arr_target[:,0])\n",
        "# all_pred_source = np.unique(np_arr_source[:,1])\n",
        "# all_pred_target = np.unique(np_arr_target[:,1])\n",
        "# all_object_source = np.unique(np_arr_source[:,2])\n",
        "# all_object_target = np.unique(np_arr_target[:,2])"
      ],
      "metadata": {
        "id": "lKyRcV62dJ4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Preprocessing"
      ],
      "metadata": {
        "id": "UDydU0Btc1MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import copy as cp\n",
        "from deep_translator import GoogleTranslator\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import RegexpParser\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stopwords = np.array(nltk.corpus.stopwords.words('english'))"
      ],
      "metadata": {
        "id": "7EINwSC2gXrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_index():\n",
        "  # getting rid of all nodes objects\n",
        "  rNode = re.compile('^n[0-9]*.*')\n",
        "\n",
        "  # getting rid of all url objects\n",
        "  rURL = re.compile(\"http://.*\")\n",
        "\n",
        "  # filtering object with no useful information in both source and target graph\n",
        "  idx_to_keep_source_obj = [i for i in range(len(np_arr_source[:,2])) if not (bool(rNode.match(np_arr_source[i,2])) or \n",
        "                                                                          bool(rURL.match(np_arr_source[i,2])))] \n",
        "  idx_to_keep_target_obj = [i for i in range(len(np_arr_target[:,2])) if not (bool(rNode.match(np_arr_target[i,2])) or \n",
        "                                                                          bool(rURL.match(np_arr_target[i,2])))]\n",
        "\n",
        "  # filtering subject with no useful information in both source and target graph\n",
        "  idx_to_keep_source_subj = [i for i in range(len(np_arr_source[:,0])) if not bool(rNode.match(np_arr_source[i,0]))] \n",
        "  idx_to_keep_target_subj = [i for i in range(len(np_arr_target[:,0])) if not bool(rNode.match(np_arr_target[i,0]))]\n",
        "\n",
        "  # keep only triplet with useful information\n",
        "  idx_to_keep_source = np.intersect1d(idx_to_keep_source_obj,idx_to_keep_source_subj)\n",
        "  idx_to_keep_target = np.intersect1d(idx_to_keep_target_obj,idx_to_keep_target_subj)\n",
        "\n",
        "  return idx_to_keep_source,idx_to_keep_target\n",
        "\n",
        "idx_to_keep_source, idx_to_keep_target = find_index()"
      ],
      "metadata": {
        "id": "rl84-5B7HCXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(idx_to_keep_source), len(idx_to_keep_target))"
      ],
      "metadata": {
        "id": "K0LNRsJ5udEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def translate(np_array):\n",
        "#   for i,s in zip(range(len(np_array)),np_array):\n",
        "#     try:\n",
        "#       np_array[i] = GoogleTranslator(source='auto', target='fr').translate(s)\n",
        "#     except:\n",
        "#       pass\n",
        "#     finally:\n",
        "#       pass\n",
        "#   return np_array\n",
        "\n",
        "\n",
        "# objet_source = translate(objet_target)\n",
        "# objet_target = translate(objet_target)\n"
      ],
      "metadata": {
        "id": "uDhxZ0u9F9rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Saving to a csv file to avoid long time running\n",
        "# df_source = pd.DataFrame(data = objet_source)\n",
        "# df_source.to_csv(\"objet_value_translated.csv\",sep=',',index=False)\n",
        "\n",
        "# df_target = pd.DataFrame(data = objet_target)\n",
        "# df_target.to_csv(\"objet_target_translated.csv\",sep=',',index=False)\n",
        "\n",
        "# #### Loading \n",
        "# df_source = pd.read_csv(\"train.csv\")\n",
        "# objet_sourcebis = df_source.values\n",
        "# objet_targetbis = df_target.values"
      ],
      "metadata": {
        "id": "8BFPV1q9F6Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(np_array):\n",
        "  for i,sentence in zip(range(len(np_array)),np_array):\n",
        "    # Removing punctuation\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    sentence = tokenizer.tokenize(sentence)\n",
        "\n",
        "    sentence = \" \".join(sentence)\n",
        "    sentence = word_tokenize(sentence)\n",
        "\n",
        "    # converting each word to lowercase\n",
        "    sentence = [word.lower() for word in sentence]\n",
        "\n",
        "    # Removing stopwords\n",
        "    sentence = [word for word in sentence if not word in stopwords]\n",
        "\n",
        "    # lemmatizer words \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
        "    \n",
        "    # Removing pronouns infinitive verbs dt...\n",
        "    for word in nltk.pos_tag(sentence):\n",
        "      word_to_keep = []\n",
        "      if word[1] not in ['IN', 'PP', 'DT','PRP$','VB','MD']:\n",
        "        word_to_keep.append(word[0])\n",
        "\n",
        "    sentence = (\" \").join(sentence)\n",
        "    np_array[i] = sentence\n",
        "  return np_array\n",
        "\n",
        "objet_source = np_arr_source[idx_to_keep_source,2]\n",
        "objet_target = np_arr_target[idx_to_keep_target,2]\n",
        "\n",
        "objet_source = preprocess(objet_source)\n",
        "objet_target = preprocess(objet_target)"
      ],
      "metadata": {
        "id": "DNSQyd_Z5xME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <Predicate, Value> Comparaison; Matching Strategies"
      ],
      "metadata": {
        "id": "DIwDedELvhuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_prop_with_occurence(np_graph):\n",
        "  \"\"\"\n",
        "  retourne toutes les propriétés avec leurs occurences respectives pour un graphe rdf donné\n",
        "  \"\"\"\n",
        "  return collections.Counter(x for x in np_graph[:,1])\n",
        "\n",
        "\n",
        "idx_to_keep_source, idx_to_keep_target = find_index()\n",
        "prop_source = np.unique(np_arr_source[idx_to_keep_source,1])\n",
        "prop_target = np.unique(np_arr_target[idx_to_keep_target,1])\n",
        "interesting_prop = np.unique(np.concatenate((prop_source,prop_target),axis=0))\n",
        "\n",
        "def main_prop(prop = interesting_prop):\n",
        "  \"\"\"\n",
        "  retourne toutes les propriétés en commun au 2 graphes classé par ordre croissant en fonction de leurs occurences\n",
        "  \"\"\"\n",
        "  prop_source = list(find_prop_with_occurence(np_arr_source).keys())\n",
        "  prop_target = list(find_prop_with_occurence(np_arr_target).keys())\n",
        "\n",
        "  # occurence of property in both target and source\n",
        "  prop_union = np.intersect1d(prop_source,prop_target)\n",
        "  dico_union = find_prop_with_occurence(np.concatenate((np_arr_source,np_arr_target),axis = 0))\n",
        "  dico_interesting_prop = dict()\n",
        "  for key in prop:\n",
        "    dico_interesting_prop[key] = dico_union[key]\n",
        "  return np.array(list({k: v for k, v in sorted(dico_interesting_prop.items(), key=lambda item: item[1],reverse = True)}.keys()))\n"
      ],
      "metadata": {
        "id": "h_S3JBCk3dBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # retourne toutes les propriétés associès à un sujet\n",
        "# f = lambda subject : np_arr_source[np_arr_source[:,0] == subject][:,1:]\n",
        "\n",
        "# source_prop_subject = dict((k, []) for k in np.unique(np_arr_source[:,0]))\n",
        "# for subject in np.unique(np_arr_source[:,0]):\n",
        "#   source_prop_subject[subject].append(f(subject))\n",
        "\n",
        "# g = lambda subject : np_arr_target[np_arr_target[:,0] == subject][:,1:]\n",
        "\n",
        "# target_prop_subject = dict((k, []) for k in np.unique(np_arr_target[:,0]))\n",
        "# for subject in np.unique(np_arr_target[:,0]):\n",
        "#   target_prop_subject[subject].append(g(subject))"
      ],
      "metadata": {
        "id": "L9yDlSBkt5qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_prop():\n",
        "  print(\"Select properties from the following list\")\n",
        "  print(main_prop)\n",
        "  print(\"WARNING Due to long time running and RAM space avalaible\")\n",
        "  print(\"We recommend that you do not use all properties on the same time but only feuw of them\")\n",
        "  print(\"Select number of property you want to choose: \")\n",
        "  n = int(input())\n",
        "  while (n<0 or n>4):\n",
        "    print(\"Not correct number of property\")\n",
        "    n = int(input())\n",
        "  print(\"Enter the desired property indexes: \")\n",
        "  arr = input() \n",
        "  l = list(map(int,arr.split(' ')))\n",
        "  return l"
      ],
      "metadata": {
        "id": "r0HUvIscoejM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_prop = main_prop()\n",
        "def choose_property(selected_prop):\n",
        "  \"\"\"\n",
        "  given a rank of occurence return\n",
        "  \"\"\"\n",
        "  # preprocessing mask where we took off all triplet with nodes and URL...\n",
        "  idx_to_keep_source, idx_to_keep_target = find_index()\n",
        "\n",
        "  props = main_prop[selected_prop]\n",
        "  print(props)\n",
        "\n",
        "  # select all subject which have one of selected property\n",
        "  idx_to_keep_source = [i for i in idx_to_keep_source if np_arr_source[i,1] in props]\n",
        "  idx_to_keep_target = [i for i in idx_to_keep_target if np_arr_target[i,1] in props]\n",
        "\n",
        "  print(\"Nombre index source conservés: \", len(idx_to_keep_source),\"Nombres index target conservés: \",len(idx_to_keep_target))\n",
        "  return idx_to_keep_source,idx_to_keep_target\n",
        "\n",
        "# choosing propertys given id based on rank of occurency\n",
        "selected_prop = input_prop() \n",
        "idx_to_keep_source,idx_to_keep_target = choose_property(selected_prop)"
      ],
      "metadata": {
        "id": "FTQl7rBX8A02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prod_obj = itertools.product(np_arr_source[idx_to_keep_source,2], np_arr_target[idx_to_keep_target,2])\n",
        "prod_obj = np.array([x for x in prod_obj])\n",
        "### \n",
        "prod_subj = itertools.product(np_arr_source[idx_to_keep_source,0], np_arr_target[idx_to_keep_target,0])\n",
        "prod_subj = np.array([x for x in prod_subj])"
      ],
      "metadata": {
        "id": "waJGcarvlDdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = len(prod_obj)"
      ],
      "metadata": {
        "id": "dMJjWYmyw_oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity Measure\n"
      ],
      "metadata": {
        "id": "7HqJTiMm2IHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install strsimpy\n",
        "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
        "from strsimpy.jaro_winkler import JaroWinkler\n",
        "from strsimpy.ngram import NGram\n",
        "from strsimpy.cosine import Cosine\n",
        "from strsimpy.metric_lcs import MetricLCS\n",
        "from strsimpy.jaccard import Jaccard\n",
        "from strsimpy.overlap_coefficient import OverlapCoefficient\n",
        "from strsimpy.sorensen_dice import SorensenDice\n",
        "from strsimpy import SIFT4"
      ],
      "metadata": {
        "id": "fKyHaR7SopRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity = lambda x,y : 1.0 if x==y else 0.0  "
      ],
      "metadata": {
        "id": "XnTkkJSdvHQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_levenshtein(x,y):\n",
        "  normalized_levenshtein = NormalizedLevenshtein()\n",
        "  return normalized_levenshtein.similarity(x, y)"
      ],
      "metadata": {
        "id": "qVT7kjlf3aAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaroWinkler(x,y):\n",
        "  jarowinkler = JaroWinkler()\n",
        "  return jarowinkler.similarity(x,y)"
      ],
      "metadata": {
        "id": "-vUk29wLnQr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(x, y,k=2):\n",
        "  jac = Jaccard(k)\n",
        "  return jac.similarity(x,y)"
      ],
      "metadata": {
        "id": "hV2hkZTrvbpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(x,y,value=2):\n",
        "  cosine = Cosine(value)\n",
        "  # avoid zero division error\n",
        "  if len(x)<2 and len(y)<2:\n",
        "    return identity(x,y)\n",
        "  return cosine.similarity(x, y)"
      ],
      "metadata": {
        "id": "A5kxZmQ-o6hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlap_coefficient_similarity(x,y):\n",
        "  over = OverlapCoefficient()\n",
        "  # avoid zero division error\n",
        "  if len(x)<3 or len(y)<3:\n",
        "    return identity(x,y)\n",
        "  return over.similarity(x,y)"
      ],
      "metadata": {
        "id": "nhV91Ss54usk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sorensen_dice_similarity(x,y):\n",
        "  sorensen = SorensenDice()\n",
        "  # avoid zero division error\n",
        "  if len(x)<3 or len(y)<3:\n",
        "    return identity(x,y)\n",
        "  return sorensen.similarity(x,y)"
      ],
      "metadata": {
        "id": "tbJlDgyC5TMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIFT4 is a general purpose string distance algorithm inspired by JaroWinkler and Longest Common Subsequence. It was developed to produce a distance measure that matches as close as possible to the human perception of string distance. Hence it takes into account elements like character substitution, character distance, longest common subsequence etc. It was developed using experimental testing, and without theoretical background.\n",
        "\n",
        "source: https://pypi.org/project/strsimpy/"
      ],
      "metadata": {
        "id": "Bnn9NILSAt-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sift4_similarity(x,y):\n",
        "  s = SIFT4()\n",
        "  return 1 - s.distance(x,y)/max(len(x),len(y))  "
      ],
      "metadata": {
        "id": "TWIjojWV0Qou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_distance_evaluation(np_cartesian_product,similarity_function):\n",
        "  mapp = np.zeros((np.shape(np_cartesian_product)[0],1))\n",
        "  for i in range(mapp.shape[0]):\n",
        "    if i%20000 == 0:\n",
        "      print((i/size)*100, \"%\")\n",
        "    mapp[i] = similarity_function(prod_obj[i,0],prod_obj[i,1])\n",
        "  return mapp"
      ],
      "metadata": {
        "id": "BqiRKsP2jMRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_mask(mapp,seuil):\n",
        "  if seuil > 1 or seuil < 0:\n",
        "    return \"Please choose value between 0.0 and 1.0\"\n",
        "  mask = mapp>seuil\n",
        "  mask = mask.flatten()\n",
        "  mask_idx = [i for i in range(np.shape(prod_obj)[0]) if mask[i]]\n",
        "  print(\"Nb element: \", len(mask_idx))\n",
        "  return mask"
      ],
      "metadata": {
        "id": "_KVpz2vIsycC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kmFeHRjMhOsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading True Alignment"
      ],
      "metadata": {
        "id": "G3bS-RgCbgyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xml.dom.minidom import parse\n",
        "DOMTree = parse('veriteTerrain.xml')"
      ],
      "metadata": {
        "id": "ZjU0-a3tAVKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = DOMTree.documentElement\n",
        "\n",
        "uriSource = collection.getElementsByTagName('entity1')\n",
        "uriTarget = collection.getElementsByTagName('entity2')\n",
        "\n",
        "verite_terrain = list()\n",
        "for uriS,uriT in zip(uriSource,uriTarget):\n",
        "  verite_terrain.append([uriS.getAttribute('rdf:resource'),uriT.getAttribute('rdf:resource')])\n",
        "\n",
        "verite_terrain = np.array(verite_terrain).astype(\"str\")"
      ],
      "metadata": {
        "id": "C0Vm-S6hCXC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj_1 = []\n",
        "obj_2 = []\n",
        "\n",
        "for subj1 in verite_terrain[:,0]:\n",
        "  obj_1.append(np_arr_source[np_arr_source[:,0] == subj1][:,2])\n",
        "\n",
        "\n",
        "for subj2 in verite_terrain[:,1]:\n",
        "  obj_2.append(np_arr_target[np_arr_target[:,0] == subj2][:,2])\n",
        "\n",
        "# for i in range(5):\n",
        "#   print(obj_1[i])\n",
        "#   print(\"\\n\")\n",
        "#   print(obj_2[i])\n",
        "#   print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "\n",
        "verite_terrain = dict(verite_terrain)"
      ],
      "metadata": {
        "id": "yfpFz0abK8yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Methods\n"
      ],
      "metadata": {
        "id": "17FmZ7OGJ2mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mapp = apply_distance_evaluation(prod_obj,jaroWinkler)\n",
        "# mask = find_mask(mapp,0.9)\n",
        "\n",
        "\n",
        "def helper_dico(mask):\n",
        "  alignement_dic = dict(prod_subj[mask])\n",
        "  alignement_source = list(alignement_dic.keys())\n",
        "  alignement_target = list(alignement_dic.values())\n",
        "  return alignement_dic,alignement_source,alignement_target\n",
        "\n",
        "# alignement_dic,alignement_source,alignement_target = helper_dico(mask)\n",
        "\n",
        "def precision(mask):\n",
        "  # parmi tous les alignements trouvés combien sont vrais\n",
        "  count = 0.0\n",
        "  for subj in prod_subj[mask][:,0]:\n",
        "    if subj in alignement_source:\n",
        "      try:\n",
        "        if alignement_dic[subj] == verite_terrain[subj]:\n",
        "          count += 1.0\n",
        "      except:\n",
        "        continue\n",
        "  return count/len(prod_subj[mask][:,0])"
      ],
      "metadata": {
        "id": "EwR3UIwyJcf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(mask):\n",
        "  # parmi tous les alignements vraies combien ont été correctement identifiés\n",
        "  count = 0.0\n",
        "  for subj in list(verite_terrain.keys()):\n",
        "    if subj in alignement_source:\n",
        "      if verite_terrain[subj] == alignement_dic[subj]:\n",
        "        count += 1.0\n",
        "  return count/len(list(verite_terrain.keys()))"
      ],
      "metadata": {
        "id": "sTCKHuC1c0CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1Score(mask):\n",
        "  if precision(mask)+recall(mask) == 0:\n",
        "    return 0.0\n",
        "  return 2*precision(mask)*recall(mask)/(precision(mask)+recall(mask))"
      ],
      "metadata": {
        "id": "ndi9ksb3Nn-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some vizualisation"
      ],
      "metadata": {
        "id": "dDGRndvIA32M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seuils = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "similaritys = [identity,normalized_levenshtein,jaroWinkler,jaccard_similarity,cosine_similarity,\n",
        "               overlap_coefficient_similarity,sorensen_dice_similarity,sift4_similarity]\n",
        "similaritys_names = [\"identity\",\"normalized_levenshtein\",\"jaroWinkler\",\"jaccard_similarity\",\"cosine_similarity\",\n",
        "               \"overlap_coefficient_similarity\",\"sorensen_dice_similarity\",\"sift4_similarity\"]\n",
        "\n",
        "\n",
        "data_precision = {sim: [] for sim in similaritys_names}\n",
        "data_recall = {sim: [] for sim in similaritys_names}\n",
        "dataf1 = {sim: [] for sim in similaritys_names}\n",
        "\n",
        "for similarity_name,similarity in zip(similaritys_names,similaritys):\n",
        "  mapp = apply_distance_evaluation(prod_obj,similarity)\n",
        "  for seuil in seuils:\n",
        "    mask = find_mask(mapp,seuil)\n",
        "    alignement_dic,alignement_source,alignement_target = helper_dico(mask)\n",
        "    dataf1[similarity_name].append(f1Score(mask))\n",
        "    data_precision[similarity_name].append(precision(mask))    \n",
        "    data_recall[similarity_name].append(recall(mask))"
      ],
      "metadata": {
        "id": "GNfkwJYgBDa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting similarities\n",
        "import matplotlib.pyplot as plt\n",
        "fig,ax = plt.subplots(3,1,figsize=(12,25))\n",
        "for similaritys_names, datalist in data_precision.items():\n",
        "    similaritys_names, datalist = zip(*data_precision.items()) \n",
        "for i in range(len(datalist)):\n",
        "  ax[0].plot(seuils, datalist[i],'-s',label=similaritys_names[i])\n",
        "ax[0].set_title(\"Precision for similarities with properties {}\".format(main_prop[selected_prop]))\n",
        "ax[0].legend(bbox_to_anchor=(1, 1),prop={'size': 16})\n",
        "ax[0].set_ylabel(\"Precision\")\n",
        "\n",
        "\n",
        "for similaritys_names, datalist in data_recall.items():\n",
        "    similaritys_names, datalist = zip(*data_recall.items()) \n",
        "for i in range(len(datalist)):\n",
        "  ax[1].plot(seuils, datalist[i],'-s',label=similaritys_names[i])\n",
        "ax[1].legend(bbox_to_anchor=(1, 1),prop={'size': 16})\n",
        "ax[1].set_title(\"Recall for similarities with properties {}\".format(main_prop[selected_prop]))\n",
        "ax[1].set_ylabel(\"Recall\")\n",
        "\n",
        "\n",
        "for similaritys_names, datalist in dataf1.items():\n",
        "    similaritys_names, datalist = zip(*dataf1.items()) \n",
        "for i in range(len(datalist)):\n",
        "  ax[2].plot(seuils, datalist[i],'-s',label=similaritys_names[i])\n",
        "ax[2].set_title(\"F1 score for similarities with properties {}\".format(main_prop[selected_prop]))\n",
        "ax[2].set_xlabel(\"Filter's Threashold\")\n",
        "ax[2].legend(bbox_to_anchor=(1, 1),prop={'size': 16})\n",
        "ax[2].set_ylabel(\"f1Score\")\n"
      ],
      "metadata": {
        "id": "mxZVuFwLbeg5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}